version: '3.8'

services:
  ollama:
    image: ollamaimgembed
    container_name: ollama_local
    environment:
      - OLLAMA_MODELS=/usr/share/ollama/.ollama/models/
    volumes:
      - type: bind
        source: /usr/share/ollama/.ollama/models/
        target: /usr/share/ollama/.ollama/models/
    entrypoint: ["sh", "-c", "mkdir -p /usr/share/ollama/.ollama/models && /bin/ollama serve"]
    runtime: nvidia
    ports:
      - "11434:11434"
    networks:
      - app_network

  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    ports:
      - "8085:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - ollama
    networks:
      - app_network

networks:
  app_network:
